#+STARTUP: inlineimages
#+STARTUP: indent
#+TODO: TODO In-Progress | Done Cancelled Failed

* Homework 4
  :PROPERTIES:
  :CUSTOM_ID: homework-4
  :END:
#+BEGIN_SRC elisp
(venv-workon "ml352")
#+END_SRC

#+RESULTS:

In =main.py= you will find an implementation of a "vanilla" policy
gradient method, applied to an MDP with a discrete action space: an
episodic version of the classic "cartpole" task. First, make sure the
provided code works on your computer by running =python main.py=. We
recommend reading through all of the code and comments in the function
=main_cartpole=, starting at the top of the function.

#+BEGIN_SRC sh :session hw4sh :results output
  main.py
#+END_SRC

The code computes some useful diagnostics, which you may find helpful to
look at while tuning hyperparameters:

-  *KL[policy before update || policy after update]*. Large spikes in KL
   divergence mean that the optimization took a large step, and
   sometimes these spikes cause a collapse in performance.
-  *Entropy of the policy*. If entropy goes down too fast, then you may
   not explore enough, but if it goes down too slowly, you'll probably
   not reach optimal performance.
-  *Explained variance of the value function*. If the value function
   perfectly explains the returns, then it will be 1; if you get a
   negative result, then it's worse than predicting a constant.

Software dependencies:

-  tensorflow (I'm using 0.10.0)
-  numpy + scipy (Anaconda recommended)
-  gym (I'm using 0.8.0, =pip install gym==0.8.0=, but old versions
   should work just as well)

** notes

*** Tasks
- continue with summaries. add
  - reward, evbefore / after
    - need to add symbolic var for them


*** tensorflow

- tensorboard --logdir=summaries/train/


*** modeling probability 

- lastest version (p2 v2) models the action distribution as normal.

- can probably change to mvn directly by using MultivariateNormalLinearOperator

- alternatively can probably create a truncated normal class similar to how diag normal is defined.



** Problem 1
   :PROPERTIES:
   :CUSTOM_ID: problem-1
   :END:

#+BEGIN_SRC python
        mean_na = dense(h2, ac_dim, weight_init=normc_initializer(0.1)) # Mean control output
        logstd_a = tf.get_variable("logstdev", [ac_dim], initializer=tf.zeros_initializer) # Variance

        sy_sampled_ac = YOUR_CODE_HERE
        sy_logprob_n = YOUR_CODE_HERE
#+END_SRC

Write a new function, =main_pendulum= that learns on the gym environment
Pendulum-v0, which has a continuous action space. In =main_cartpole=,
note that the neural network outputs "logits" (i.e., log-probabilities
plus-or-minus a constant) that specify a categorical distribution. On
the other hand, for the pendulum task, your neural network should output
the mean of a Gaussian distribution, a separate parameter vector to
parameterize the log standard deviation. For example, you could use the
following code:

You should also compute differential entropy (replacing =sy_ent=) and
KL-divergence (=sy_kl=) for the Gaussian distribution.

The pendulum problem is slightly harder, and using a fixed stepsize does
not work reliably---thus, we instead recommend using an adaptive stepsize, where you adjust it based on the KL divergence between the new and old policy. Code for this stepsize adaptation is provided.

You can plot your results using the script =plot_learning_curves.py= or
your own plotting code.

*Deliverables*

-  Show a plot with the pendulum converging to EpRewMean of at least =-300=. Include EpRewMean, KL, Entropy in your plots.

-  Describe the hyperparameters used and how many timesteps your algorithm took to learn.

*** Tasks

**** current

- Google open ai Pendulum-v0 implementations


**** future

- recurrence

- trpo / GAE

- move on to next question

- truncate the gradient?

- can we sample better

  - truncated normal?

- are there other problems with sampling

  - are we applying actions correctly

- are the correct log_prob being calculated?

- is surr loss being calculated correctly?

- should stepsize be different?

- refs

  - other trpo implementations?

  - [[https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/Continuous%2520MountainCar%2520Actor%2520Critic%2520Solution.ipynb][d britz]]


**** refs

- by searching kl penalized policy gradient tensorflow

  - [[http://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html][deep policy gradient]]


** Problem 2
   :PROPERTIES:
   :CUSTOM_ID: problem-2
   :END:

tensorboard
#+BEGIN_SRC sh :session p2sh
  tensorboard --logdir=summaries/p2/
#+END_SRC

1. Implement a neural network value function with the same interface as
   =LinearVF=. Add it to the provided cartpole solver, and compare the
   performance of the linear and neural network value function (i.e.,
   baseline).
2. Perform the same comparison--linear vs neural network--for your
   pendulum solver from Problem 1. You should be able to obtain faster
   learning using the neural network.

*Deliverables*

-  A comparison of linear vs neural network value function on the
   cartpole. Show the value function's explained variance (EVBefore) and
   mean episode reward (EpRewMean).
-  A comparison of linear vs neural network value function on the
   pendulum. Show the value function's explained variance (EVBefore) and
   mean episode reward (EpRewMean).

In both cases, list the hyperparameters used for neural network
training.

*** notes

**** debugging flat std/b and mean/b

***** pdb looking through hidden
(pdb) b 534, total_timesteps>2e4
find:
#+BEGIN_SRC python
  sy_h1_a = tf.get_default_graph().get_tensor_by_name("policy_function/mean_h1_a:0")
  sy_mean_w = tf.get_default_graph().get_tensor_by_name("policy_function/mean/w_1:0")
  sy_mean_b = tf.get_default_graph().get_tensor_by_name("policy_function/mean/b_1:0")
#+END_SRC

views
#+BEGIN_SRC python
  sess.run(tf.reduce_sum(sy_h1_a,axis=1), feed_dict = {pf.sy_ob_no:ob_no})  
#+END_SRC

what should be mean:
#+BEGIN_SRC python
  p1 = sess.run(tf.matmul(sy_h1_a,sy_mean_w)+sy_mean_b, feed_dict = {pf.sy_ob_no:ob_no})
#+END_SRC

what is mean:
#+BEGIN_SRC python
  p2 = sess.run(pf.sy_y_pred[0], feed_dict = {pf.sy_ob_no:ob_no})
#+END_SRC

difference:
#+BEGIN_SRC python
  np.sum(np.square(p1-p2))
#+END_SRC



** Problem 3 (bonus)
   :PROPERTIES:
   :CUSTOM_ID: problem-3-bonus
   :END:

Implement a more advanced policy gradient method from lecture (such as
TRPO, or the advantage function estimator used in A3C or generalized
advantage estimation), and apply it to the gym environment =Hopper-v1=.
See if you can learn a good gait in less than 500,000 timesteps. Hint:
it may help to standardize your inputs using a running estimate of mean
and standard deviation.

#+BEGIN_EXAMPLE
    ob_rescaled = (ob_raw - mean) / (stdev + epsilon)
#+END_EXAMPLE

*Deliverables*

A description of what you implemented, and learning curves on the
Hopper-v1 environment.
